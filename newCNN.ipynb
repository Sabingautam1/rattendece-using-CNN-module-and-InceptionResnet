{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7023b33-2a42-4c97-88c0-81a11009bacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8167, Validation Loss: 0.0323, Accuracy: 73.91%\n",
      "Epoch [2/50], Loss: 0.4090, Validation Loss: 0.0152, Accuracy: 100.00%\n",
      "Epoch [3/50], Loss: 0.1499, Validation Loss: 0.0059, Accuracy: 100.00%\n",
      "Epoch [4/50], Loss: 0.0582, Validation Loss: 0.0027, Accuracy: 100.00%\n",
      "Epoch [5/50], Loss: 0.0055, Validation Loss: 0.0008, Accuracy: 100.00%\n",
      "Epoch [6/50], Loss: 0.0239, Validation Loss: 0.0004, Accuracy: 100.00%\n",
      "Epoch [7/50], Loss: 0.0037, Validation Loss: 0.0016, Accuracy: 100.00%\n",
      "Epoch [8/50], Loss: 0.0048, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [9/50], Loss: 0.0010, Validation Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [10/50], Loss: 0.0002, Validation Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [11/50], Loss: 0.0965, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [12/50], Loss: 0.0011, Validation Loss: 0.0024, Accuracy: 95.65%\n",
      "Epoch [13/50], Loss: 0.3507, Validation Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [14/50], Loss: 0.0017, Validation Loss: 0.0108, Accuracy: 91.30%\n",
      "Epoch [15/50], Loss: 0.0008, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [16/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [17/50], Loss: 0.0024, Validation Loss: 0.0012, Accuracy: 100.00%\n",
      "Epoch [18/50], Loss: 0.0060, Validation Loss: 0.0040, Accuracy: 95.65%\n",
      "Epoch [19/50], Loss: 0.0052, Validation Loss: 0.0028, Accuracy: 95.65%\n",
      "Epoch [20/50], Loss: 0.0005, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [21/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [22/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [23/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [24/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [25/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [26/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [27/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [28/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [29/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [30/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [31/50], Loss: 0.0001, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [32/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [33/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [34/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [35/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [36/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [37/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [38/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [39/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [40/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [41/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [42/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [43/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [44/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [45/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [46/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [47/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [48/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [49/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [50/50], Loss: 0.0000, Validation Loss: 0.0000, Accuracy: 100.00%\n",
      "Model saved successfully at: cnn_model_20240302224000.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from datetime import datetime\n",
    "from tkinter import *\n",
    "from tkinter import font\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import tkinter.simpledialog as sd\n",
    "from ttkthemes import ThemedStyle\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Step 1: Dataset Preparation\n",
    "dataset_path = 'C:\\\\Users\\\\SABIN\\\\Desktop\\\\onlycnn\\\\images'\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Step 3: Model Definition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # Adjust the input size based on your image dimensions\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 32 * 16 * 16)  # Adjust the size based on your image dimensions\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 4: Model Initialization\n",
    "num_classes = len(dataset.classes)\n",
    "cnn_model = SimpleCNN(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 5: Training Loop\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in DataLoader(val_dataset, batch_size=batch_size, shuffle=False):\n",
    "            val_outputs = cnn_model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            correct_predictions += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataset)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Step 6: Save the Model\n",
    "model_save_path = f'cnn_model_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.pth'\n",
    "\n",
    "\n",
    "try:\n",
    "    torch.save(cnn_model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved successfully at: {model_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5af2b99-a774-44f6-84e5-b11fbbd964e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SimpleCNN:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([4, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Load the trained weights\u001b[39;00m\n\u001b[0;32m     51\u001b[0m model_weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/SABIN/Desktop/onlycnn/cnn_model_20240302222228.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mcnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#assuming\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SimpleCNN:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([4, 128]) from checkpoint, the shape in current model is torch.Size([3, 128]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([3])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Step 1: Load the trained CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # Adjust the input size based on your image dimensions\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "    \n",
    "    # Print the shape of x to understand its dimensions\n",
    "        print(x.shape)\n",
    "    \n",
    "    # Dynamically adjust the size based on the shape of x\n",
    "        x = x.view(x.size(0), -1)\n",
    "    \n",
    "    # Adjust the size of fc1 based on the number of features in x\n",
    "        self.fc1 = nn.Linear(x.size(1), 128)\n",
    "    \n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Assuming your CNN model is named 'cnn_model'\n",
    "cnn_model = SimpleCNN(num_classes=num_classes)\n",
    "\n",
    "# Load the trained weights\n",
    "model_weights_path = 'C:/Users/SABIN/Desktop/onlycnn/cnn_model_20240302224000.pth'\n",
    "cnn_model.load_state_dict(torch.load(model_weights_path))\n",
    "cnn_model.eval()\n",
    "#assuming\n",
    "your_input_width = 100\n",
    "your_input_height = 100\n",
    "# Function to open the file and perform face recognition\n",
    "\n",
    "\n",
    "def open_file():\n",
    "    dataset = datasets.ImageFolder('C:\\\\Users\\\\SABIN\\\\Desktop\\\\onlycnn\\\\images')  # Provide the correct path\n",
    "    idx_to_class = {i: c for c, i in dataset.class_to_idx.items()}\n",
    "\n",
    "    def collate_fn(x):\n",
    "        return x[0]\n",
    "\n",
    "    loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "    name_list = []  \n",
    "    embedding_list = []\n",
    "\n",
    "    for img, idx in loader:\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        \n",
    "        # Use your CNN model for face recognition\n",
    "        with torch.no_grad():\n",
    "            outputs = cnn_model(img_tensor.unsqueeze(0))\n",
    "        \n",
    "        emb = outputs.detach()\n",
    "        embedding_list.append(emb)\n",
    "        name_list.append(idx_to_class[idx])\n",
    "\n",
    "    # Save data\n",
    "    data = [embedding_list, name_list]\n",
    "    torch.save(data, 'data.pt')  # Saving data.pt file\n",
    "\n",
    "    # Create or open the CSV file for attendance\n",
    "    with open('recog.csv', mode='w', newline='') as csvfile:\n",
    "        fieldnames = ['Name', 'Timestamp']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "\n",
    "        cam = cv2.VideoCapture(0)\n",
    "        written_names = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cam.read()\n",
    "            if not ret:\n",
    "                print(\"Fail to grab frame, try again\")\n",
    "                break\n",
    "\n",
    "            # Perform face detection using OpenCV\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Extract the face from the frame\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                # Resize the face to match the input size of your CNN\n",
    "                face = cv2.resize(face, (your_input_width, your_input_height))\n",
    "\n",
    "                # Convert the face to a PyTorch tensor\n",
    "                img_tensor = transforms.ToTensor()(Image.fromarray(face))\n",
    "\n",
    "                # Use your CNN model for face recognition\n",
    "                with torch.no_grad():\n",
    "                    outputs = cnn_model(img_tensor.unsqueeze(0))\n",
    "\n",
    "                emb = outputs.detach()\n",
    "                dist_list = []  # list of matched distances, minimum distance is used to identify the person\n",
    "\n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list)  # get minimum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist)  # get minimum dist index\n",
    "                name = name_list[min_dist_idx]  # get name corresponding to minimum dist\n",
    "\n",
    "                if min_dist < 0.90 and name not in written_names:\n",
    "                    # Draw bounding box and label on the face\n",
    "                    frame = cv2.putText(frame, f\"{name} {min_dist:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                        (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    frame = cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "                    written_names.append(name)\n",
    "\n",
    "                    # Write attendance data to CSV\n",
    "                    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    writer.writerow({'Name': name, 'Timestamp': timestamp})\n",
    "\n",
    "            cv2.imshow(\"IMG\", frame)\n",
    "\n",
    "            k = cv2.waitKey(1)\n",
    "            if k % 256 == 27:  # ESC\n",
    "                print('Esc pressed, closing...')\n",
    "                break\n",
    "\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "                 \n",
    "def show():\n",
    "    loading_frame = tk.Frame(root, bg=\"#f7f7f7\")\n",
    "    loading_frame.pack(side=tk.BOTTOM)\n",
    "\n",
    "    # Create a label to display the animation\n",
    "    loading_label = tk.Label(loading_frame, text=\"Loading...\", font=(\"Helvetica\", 18), bg=\"#f7f7f7\")\n",
    "    loading_label.pack(padx=20, pady=20)\n",
    "\n",
    "    # Use the after method to delay showing the animation for 2 seconds\n",
    "    root.after(2000, lambda: loading_frame.pack_forget())\n",
    "    # Replace this line with the code to display your data\n",
    "    # For example:\n",
    "    # data_label = tk.Label(root, text=\"Here's your data!\", font=(\"Helvetica\", 18))\n",
    "    # data_label.pack(padx=20, pady=20)\n",
    "    os.startfile('C:\\\\Users\\\\SABIN\\\\Desktop\\\\onlycnn\\\\recog.csv')\n",
    "    \n",
    "\n",
    "def help():\n",
    "    help_text = \"\"\"\n",
    "    Welcome to the Swastik College Attendance System!\n",
    "\n",
    "    This program is designed to help you keep track of attendance for your classes.\n",
    "\n",
    "    To get started, simply click the Start button and select the file containing your class roster.\n",
    "\n",
    "    Once you've selected the file, the program will automatically detect the names of your students and begin tracking attendance.\n",
    "\n",
    "    Use the Stop button to end attendance tracking and save the results to a file.\n",
    "\n",
    "    You can also use the Show button to display a summary of attendance data for your class.\n",
    "\n",
    "    If you have any questions or issues, please don't hesitate to contact the support team at support@swastik.edu.np.\n",
    "    \"\"\"\n",
    "    messagebox.showinfo(\"Help\", help_text)\n",
    "    import os\n",
    "def add():\n",
    "      \n",
    "    # Load the Haar Cascade face classifier\n",
    "    face_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\SABIN\\\\Downloads\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Initialize the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Prompt user for the person's name\n",
    "    person_name = sd.askstring(\"Add Person\", \"Enter the person's name:\")\n",
    "    \n",
    "    # Check if person_name is None (user clicked Cancel or closed the input prompt)\n",
    "    if person_name is not None:\n",
    "        # Add the person to the listbox\n",
    "        listbox.insert(tk.END, person_name)\n",
    "\n",
    "        # Create a folder for the person's face images\n",
    "        folder_path = os.path.join('C:\\\\Users\\\\SABIN\\\\Desktop\\\\onlycnn\\\\images', person_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        # Capture 50 face images\n",
    "        face = None\n",
    "        count = 0\n",
    "        while count < 50:\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Convert the frame to grayscale\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the grayscale frame\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "            # Iterate over each detected face and save it as an image\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Extract the face from the frame\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Resize the face to 100x100 pixels\n",
    "                face = cv2.resize(face, (100, 100))\n",
    "\n",
    "                # Save the face as an image in the person's folder\n",
    "                cv2.imwrite(os.path.join(folder_path, 'face_{}.jpg'.format(count)), face)\n",
    "\n",
    "                # Increment the count of captured faces\n",
    "                count += 1\n",
    "\n",
    "            # Display the captured face\n",
    "            cv2.imshow('Captured Face', face)\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "\n",
    "        # Close all windows\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "       \n",
    "root = tk.Tk()\n",
    "\n",
    "# Set the window title and icon\n",
    "root.title(\"7TH Project\")\n",
    "root.geometry(\"800x500\")\n",
    "\n",
    "# Set the window background color\n",
    "root.config(bg=\"#cfe4f7\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a top frame with a label\n",
    "top_frame = tk.Frame(root, height=100)\n",
    "top_frame.pack(fill=tk.X)\n",
    "\n",
    "# Define gradient colors\n",
    "gradient_colors = (\"#34495E\", \"#2C3E50\")\n",
    "\n",
    "# Create a canvas for the gradient background\n",
    "gradient_canvas = tk.Canvas(top_frame, width=root.winfo_screenwidth(), height=100, bg=\"#2C3E50\", highlightthickness=0)\n",
    "gradient_canvas.place(x=0, y=0)\n",
    "\n",
    "# Create gradient rectangle\n",
    "gradient_canvas.create_rectangle(0, 0, root.winfo_screenwidth(), 100, fill=\"\", outline=\"\")\n",
    "\n",
    "# Add gradient colors to rectangle\n",
    "for i in range(len(gradient_colors)):\n",
    "    gradient_canvas.create_line(0, i, root.winfo_screenwidth(), i, fill=gradient_colors[i], width=1)\n",
    "    \n",
    "# Create a label on the top frame\n",
    "title_label = tk.Label(top_frame, text=\"Attendance System\", font=(\"Helvetica\", 30), bg=\"#2C3E50\", fg=\"#ECF0F1\")\n",
    "title_label.place(relx=0.5, rely=0.5, anchor=\"center\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a frame for the logo image\n",
    "logo_frame = tk.Frame(root, bg=\"#cfe4f7\")\n",
    "logo_frame.pack()\n",
    "\n",
    "# Load the image using PIL\n",
    "image = Image.open(\"C:\\\\Users\\\\SABIN\\\\Desktop\\\\logo.jpg\")\n",
    "\n",
    "# Scale the image to double the size\n",
    "width, height = image.size\n",
    "image = image.resize((int(width*0.8), int(height*0.8)))\n",
    "\n",
    "# Create a PhotoImage object from the image\n",
    "photo = ImageTk.PhotoImage(image)\n",
    "\n",
    "# Add the image to the label in the logo frame\n",
    "logo_label=tk.Label(logo_frame, image=photo, bg=\"#cfe4f7\")\n",
    "logo_label.pack(pady=20)\n",
    "\n",
    "text_label = tk.Label(logo_frame, text=\"\"\"7th SEMESTER PROJECT ON\\nAutomated Attendance System using CNN\\nSwastik College\"\"\",\n",
    "                      font=(\"Helvetica\", 18), bg=\"#cfe4f7\", fg=\"#283142\", justify=\"center\", padx=20)\n",
    "text_label.pack()\n",
    "\n",
    "\n",
    "#Create a frame for the buttons\n",
    "button_frame = tk.Frame(root, bg=\"#cfe4f7\")\n",
    "button_frame.pack()\n",
    "\n",
    "# Create three buttons in the button frame\n",
    "start_button = tk.Button(button_frame, text=\"Start\", width=10, height=4, bg=\"#00a896\", fg=\"#f7f7f7\", font=(\"Helvetica\", 18), command=open_file)\n",
    "start_button.pack(side=tk.LEFT, padx=20, pady=20)\n",
    "\n",
    "show_button = tk.Button(button_frame, text=\"Show\", width=10, height=4, bg=\"#283142\", fg=\"#f7f7f7\", font=(\"Helvetica\", 18), command=show)\n",
    "show_button.pack(side=tk.LEFT, padx=20, pady=20)\n",
    "\n",
    "help_button = tk.Button(button_frame, text=\"Help\", width=10, height=4, bg=\"#00a896\", fg=\"#f7f7f7\", font=(\"Helvetica\", 18), command=help)\n",
    "help_button.pack(side=tk.LEFT, padx=20, pady=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the listbox\n",
    "listbox = tk.Listbox(root)\n",
    "listbox.pack()\n",
    "\n",
    "add_button = tk.Button(button_frame, text=\"Add\", width=10, height=4, bg=\"#00a896\", fg=\"#f7f7f7\", font=(\"Helvetica\", 18), command=add)\n",
    "add_button.pack(side=tk.LEFT, padx=20, pady=20)\n",
    "\n",
    "\n",
    "# Make the window resizable\n",
    "root.resizable(True, True)\n",
    "\n",
    "# Set the minimum window size\n",
    "root.minsize(800, 500)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa881dc-a123-4cef-aabe-6e5413340930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
